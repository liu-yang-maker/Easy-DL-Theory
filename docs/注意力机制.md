注意力机制

记忆网络





# 注意力机制

### 人类的注意力机制

<img src="./PIC/Attention/3.png" alt="3" style="zoom:50%;" />

在深度学习中注意力机制的研究：

<img src="./PIC/Attention/1.png" alt="1" style="zoom:50%;" />

参考：
[1] Recurrent Models of Visual Attention. NIPS 2014: 2204- 2212
[2] Neural machine translation by jointly learning to align and translate, ICLR 2015
[3] Show, Attend and Tell: Neural Image Caption Generation with Visual Attention，ICML 2015
[4] Attention is all you need，NIPS 2017

### Recurrent Models of Visual Attention

...

### 注意力机制在在神经机器翻译领域的应用

- 神经机器翻译主要以Encoder-Decoder模型为基础结构

<img src="./PIC/Attention/4.png" alt="4" style="zoom:50%;" />

- 在神经机器翻译中，Encoder一般采用RNN或者LSTM实现
  - 从统计角度，翻译相当于寻找译句y，使得给定原句 x 时条件概率最大：$\arg \max _{y} p(\boldsymbol{y} \mid \boldsymbol{x})$
  - 得到上下文向量 c 的方法有很多，可以直接将最后一个隐状态作为上下文变量，也可对最后的隐状态进行一个非线性变换 $σ(⋅)$，或对所有的隐状态进行非线性变换 $σ(⋅)$

$$
\begin{aligned} c &=h_{T} \\ h_{t}=f\left(x_{t}, h_{t-1}\right) \,\,\,\  c &=\sigma\left(h_{T}\right) \\ c &=\sigma\left(h_{1}, h_{2}, \cdots, h_{T}\right) \end{aligned}
$$

### 解码器

- 用给定的上下文向量**c**和之前已经预测的词 $\{y_1,...,y_{t-1}\}$

<img src="./PIC/Attention/5.png" alt="5" style="zoom:50%;" />

### 现存问题

-  输入序列不论长短都会被编码成一个固定长度的向量表示，而解码则受限于该固定长度的向量表示
- 这个问题限制了模型的性能，尤其是当输入序列比较长时，模型的性能会变得很差

### 神经网络模型注意力机制

...

### 注意力洗漱的计算

...

### 几种主流的注意力机制

<img src="./PIC/Attention/6.png" alt="6" style="zoom:50%;" />

### 注意力机制的理解

- Attention函数的本质可以被描述为一个查询(query)到一 系列(键key-值value)对的映射

<img src="./PIC/Attention/7.png" alt="7" style="zoom:50%;" />
$$
\text { Attention(Query, Source })=\sum_{i=1}^{L_{x}} \text { similarity }\left(\text { Query }, \mathrm{Key}_{i}\right){\times} \text { Value }_{i}
$$






















# 记忆网络

### 产生背景

现代计算机的一个巨大优势就是可以对信息进行存储。但 是，大多数机器学习模型缺少这种可以读取、写入的长期 记忆的内存结构。RNN、LSTM这样的神经网络原则上可以实现记忆存储， 但是，它们由隐藏状态和权重编码包含的记忆太小了，不能记忆足够信息。基于此，Facebook AI Research提出了一种用于问答任务的记忆网络，实现了记忆的存储。广义上讲，循环神经网络也是记忆网络的一种

### 结构

结构示意图： I, G, O, R四个模块

<img src="./PIC/Attention/2.png" alt="2" style="zoom:50%;" />

参考：Weston, Jason, Sumit Chopra, and Antoine Bordes. "Memory networks." arXiv preprint arXiv:1410.3916 (2014).

- Input vector：将输入x(字符、单词、句子等不同的粒度)转成内部特征向量的表示 $ I(x)$
- Generalization：根据新的输入更新记忆单元中的memory slot $𝒎_𝑖$，$\boldsymbol{m}_{\boldsymbol{i}}=G\left(\boldsymbol{m}_{\boldsymbol{i}}, \boldsymbol{I}(\boldsymbol{x}), \boldsymbol{m}\right), \forall i$.

- Output feature map：根据记忆单元和新的输入，输出特征 $𝐨 = O (x’, 𝒎)$

- Response：最后，解码输出特征o，并给出对应的响应 $𝑟 = 𝑅 (𝐨)$，R可以是一个RNN网络生成回答的句子，更简单的话可以计算相关分数，比如W是一个单词集合(dictionary)











































